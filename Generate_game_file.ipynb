{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Generate_game_file.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1VRgvBJwaX0mF6dv9SvYE5RCLq3EqMXy4","authorship_tag":"ABX9TyMdvAz0+DI24pNq5OLY+tYr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"3a1bc344451d4cd583130d7f1af0c3e9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_2625562a9167427f80f3e95880b40d55","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_5e2766d4c4d54c8a894a231975e0f92c","IPY_MODEL_907cfedb4a9a4f71ab2da34bec24a647","IPY_MODEL_e9f003dab6ef4a50a6b4f94e35035c99"]}},"2625562a9167427f80f3e95880b40d55":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5e2766d4c4d54c8a894a231975e0f92c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b78af9050ff345d0a0270488b79a9210","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_94ef21454bd54f7ab18649e26753f5c3"}},"907cfedb4a9a4f71ab2da34bec24a647":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_d201a03b15ba4b0aaff67778030e59f3","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1789735,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1789735,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_957c0df153804cf5940a1ee0513663ed"}},"e9f003dab6ef4a50a6b4f94e35035c99":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_38d3c9389d5b4afcbd21cea08fcc1429","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.71M/1.71M [00:00&lt;00:00, 3.85MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2983f400eee6491391c311db9a0ecea0"}},"b78af9050ff345d0a0270488b79a9210":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"94ef21454bd54f7ab18649e26753f5c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d201a03b15ba4b0aaff67778030e59f3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"957c0df153804cf5940a1ee0513663ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"38d3c9389d5b4afcbd21cea08fcc1429":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2983f400eee6491391c311db9a0ecea0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","source":["%cd /content/drive/MyDrive/Colab Notebooks/OxHack20\n","!pip install yolov5\n","!pip install face_detection"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"glFVXDCYXU-H","executionInfo":{"status":"ok","timestamp":1645951167361,"user_tz":0,"elapsed":27496,"user":{"displayName":"scott huang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhHOANt4Vlu9JS3VBKWPOfNJfvVYqI1ySmaWmRk=s64","userId":"06489357941762676089"}},"outputId":"3939e82c-f86e-4dea-e9c5-cbcb339499bd"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/OxHack20\n","Collecting yolov5\n","  Downloading yolov5-6.0.6-py36.py37.py38-none-any.whl (837 kB)\n","\u001b[K     |████████████████████████████████| 837 kB 5.0 MB/s \n","\u001b[?25hRequirement already satisfied: torchvision>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from yolov5) (0.11.1+cu111)\n","Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from yolov5) (4.62.3)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from yolov5) (0.11.2)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from yolov5) (1.21.5)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from yolov5) (1.4.1)\n","Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.7/dist-packages (from yolov5) (3.2.2)\n","Collecting fire\n","  Downloading fire-0.4.0.tar.gz (87 kB)\n","\u001b[K     |████████████████████████████████| 87 kB 6.3 MB/s \n","\u001b[?25hRequirement already satisfied: opencv-python>=4.1.2 in /usr/local/lib/python3.7/dist-packages (from yolov5) (4.1.2.30)\n","Requirement already satisfied: tensorboard>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from yolov5) (2.8.0)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.7/dist-packages (from yolov5) (1.3.5)\n","Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from yolov5) (7.1.2)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.7/dist-packages (from yolov5) (2.23.0)\n","Collecting thop\n","  Downloading thop-0.0.31.post2005241907-py3-none-any.whl (8.7 kB)\n","Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from yolov5) (1.10.0+cu111)\n","Collecting boto3>=1.19.1\n","  Downloading boto3-1.21.8-py3-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 46.4 MB/s \n","\u001b[?25hCollecting sahi>=0.8.9\n","  Downloading sahi-0.9.0-py3-none-any.whl (78 kB)\n","\u001b[K     |████████████████████████████████| 78 kB 6.6 MB/s \n","\u001b[?25hCollecting PyYAML>=5.3.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 39.8 MB/s \n","\u001b[?25hCollecting s3transfer<0.6.0,>=0.5.0\n","  Downloading s3transfer-0.5.2-py3-none-any.whl (79 kB)\n","\u001b[K     |████████████████████████████████| 79 kB 6.0 MB/s \n","\u001b[?25hCollecting botocore<1.25.0,>=1.24.8\n","  Downloading botocore-1.24.8-py3-none-any.whl (8.6 MB)\n","\u001b[K     |████████████████████████████████| 8.6 MB 39.5 MB/s \n","\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n","  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n","Collecting urllib3<1.27,>=1.25.4\n","  Downloading urllib3-1.26.8-py2.py3-none-any.whl (138 kB)\n","\u001b[K     |████████████████████████████████| 138 kB 46.3 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.25.0,>=1.24.8->boto3>=1.19.1->yolov5) (2.8.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->yolov5) (3.0.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->yolov5) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->yolov5) (1.3.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.4->yolov5) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.25.0,>=1.24.8->boto3>=1.19.1->yolov5) (1.15.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->yolov5) (2021.10.8)\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 51.2 MB/s \n","\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->yolov5) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->yolov5) (2.10)\n","Requirement already satisfied: shapely>=1.8.0 in /usr/local/lib/python3.7/dist-packages (from sahi>=0.8.9->yolov5) (1.8.1)\n","Collecting opencv-python>=4.1.2\n","  Downloading opencv_python-4.5.5.62-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.4 MB)\n","\u001b[K     |████████████████████████████████| 60.4 MB 1.2 MB/s \n","\u001b[?25hCollecting Pillow>=7.1.2\n","  Downloading Pillow-9.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n","\u001b[K     |████████████████████████████████| 4.3 MB 40.2 MB/s \n","\u001b[?25hCollecting terminaltables\n","  Downloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->yolov5) (0.4.6)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->yolov5) (57.4.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->yolov5) (0.37.1)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->yolov5) (1.43.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->yolov5) (1.0.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->yolov5) (1.35.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->yolov5) (1.0.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->yolov5) (0.6.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->yolov5) (3.3.6)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->yolov5) (1.8.1)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->yolov5) (3.17.3)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->yolov5) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->yolov5) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->yolov5) (4.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->yolov5) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.4.1->yolov5) (4.11.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.4.1->yolov5) (3.7.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.4.1->yolov5) (3.10.0.2)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.4.1->yolov5) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->yolov5) (3.2.0)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from fire->yolov5) (1.1.0)\n","Building wheels for collected packages: fire\n","  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115942 sha256=d9308373b7731f294176fd96693daa6646675ac395a548db27b73496d264d6b1\n","  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n","Successfully built fire\n","Installing collected packages: urllib3, jmespath, botocore, terminaltables, s3transfer, PyYAML, Pillow, opencv-python, fire, thop, sahi, boto3, yolov5\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","  Attempting uninstall: PyYAML\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","  Attempting uninstall: Pillow\n","    Found existing installation: Pillow 7.1.2\n","    Uninstalling Pillow-7.1.2:\n","      Successfully uninstalled Pillow-7.1.2\n","  Attempting uninstall: opencv-python\n","    Found existing installation: opencv-python 4.1.2.30\n","    Uninstalling opencv-python-4.1.2.30:\n","      Successfully uninstalled opencv-python-4.1.2.30\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Successfully installed Pillow-9.0.1 PyYAML-6.0 boto3-1.21.8 botocore-1.24.8 fire-0.4.0 jmespath-0.10.0 opencv-python-4.5.5.62 s3transfer-0.5.2 sahi-0.9.0 terminaltables-3.1.10 thop-0.0.31.post2005241907 urllib3-1.25.11 yolov5-6.0.6\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["PIL"]}}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting face_detection\n","  Downloading face_detection-0.2.2.tar.gz (20 kB)\n","Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.7/dist-packages (from face_detection) (1.10.0+cu111)\n","Requirement already satisfied: torchvision>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from face_detection) (0.11.1+cu111)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from face_detection) (1.21.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6->face_detection) (3.10.0.2)\n","Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.3.0->face_detection) (9.0.1)\n","Building wheels for collected packages: face-detection\n","  Building wheel for face-detection (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for face-detection: filename=face_detection-0.2.2-py3-none-any.whl size=25574 sha256=31068a30d51428ed5338f9eaf3a127d66363566ce5084dbd1dff6a49cc5a18b6\n","  Stored in directory: /root/.cache/pip/wheels/dd/2d/84/c7a9ce1760b00f8b1d431c9564b108acfb98f588c18d4a1a9c\n","Successfully built face-detection\n","Installing collected packages: face-detection\n","Successfully installed face-detection-0.2.2\n"]}]},{"cell_type":"markdown","source":["This function uses 2 pre-trained models: Yolo version 5 for human object detection, and RetinaNetMobile for face detection. \n","\n","The **input** of function is any lecture video, preferable from Oxford Mathematics Institute Lecture Theatre 1. \n","\n","The **output** of function is a json file indicating timestamp, detected lecturer, and his/her face:\n","\n","```\n","file name: videoname.json\n","{\n","  time_stamp: index i frame number\n","  boss_position: [xmin, xmax, ymin, ymax]\n","  boss_status: True/False.     True indicate there is face on line\n","  face_status: (-1,-1,-1,-1) if none, otherwise gives [xmin,xmax,ymin,ymax]\n","}\n","\n","\n","```\n","\n"],"metadata":{"id":"orHKtNoIT6P5"}},{"cell_type":"code","source":["import yolov5\n","import cv2\n","import json\n","import face_detection\n","import warnings\n","\n","def check_inside(box1,box2,epsilon = 3):\n","  #box in form [x1, x2, y1, y2]\n","  # return True if box1 totally inside box2\n","  ax1,ax2,ay1,ay2 = box1\n","  bx1,bx2,by1,by2 = box2\n","  return (ax1+3 >= bx1 and ax2 <= bx2+3 and ay1+3>=by1 and ay2<=by2+3)\n","\n","def transform_face_box(box_pre):\n","  #box_pre in form # x1, y1, x2, y2,conf\n","  #return box in form [x1,x2,y1,y2]\n","  x1, y1, x2, y2,conf = box_pre\n","  return [x1,x2,y1,y2]\n","\n","def transform_man_box(box_pre):\n","  #box_pre in form # x1, y1, x2, y2\n","  #return box in form [x1,x2,y1,y2]\n","  x1, y1, x2, y2 = box_pre\n","  return [x1,x2,y1,y2]\n","\n","def generate_game_file(video_path):\n","  # Output a json file in form\n","  '''\n","    videoname.json\n","    {\n","    time_stamp: index i frame number\n","    boss_position: [x1, x2, y1, y2]\n","    boss_status: True/False.     True indicate there is face on line\n","    face_status: (-1,-1,-1,-1) if none, otherwise gives [x1,x2,y1,y2]\n","    }\n","\n","  '''\n","  # load model of human detection \n","  model = yolov5.load('yolov5s.pt')\n","  detector = face_detection.build_detector(\n","  \"RetinaNetMobileNetV1\", confidence_threshold=.5, nms_iou_threshold=.3)\n","  videoCap = cv2.VideoCapture(video_path)\n","  fps = 30 #round(videoCap.get(cv2.CAP_PROP_FPS))\n","  # load the pre-trained model of face detection\n","  \n","  resol_width = videoCap.get(cv2.CAP_PROP_FRAME_WIDTH)\n","  resol_height = cv2.CAP_PROP_FRAME_HEIGHT\n","\n","  \n","  # for every one frame\n","  frame_count = 0\n","  time_stamp = 0\n","  jsonResolu = (resol_width,resol_height)\n","  jsonList = []\n","  gap = 2\n","  with open('json_data.json', 'w') as outfile:\n","    while True:\n","      ret, img=videoCap.read()\n","      if ret:\n","        frame_count+=1\n","        if (frame_count*gap)%fps == 0:\n","            # perform inference\n","            results = model(img)\n","            # parse results\n","            predictions = results.pred[0]\n","            boxes = (predictions[:, :4]).tolist() # x2, x1, y2, y1\n","            scores = (predictions[:, 4]).tolist()\n","            categories = (predictions[:, 5]).tolist()\n","            # Initialize the holder\n","            man_bbox = [-1,-1,-1,-1]\n","            face_bbox = [-1,-1,-1,-1]\n","            face_front = False\n","            #Extract the prof prosition\n","            if 0.0 in categories:\n","              # if we detect the prof\n","              man_index = scores.index(max([scores[i] for i, e in enumerate(categories) if e == 0.0]))\n","              man_bbox = transform_man_box(boxes[man_index]) # x1, y1, x2, y2\n","              \n","              face_bboxes_pre = detector.detect(img).tolist() \n","              face_bboxes = [transform_face_box(item) for item in face_bboxes_pre]\n","              \n","\n","              for item in face_bboxes:\n","                if check_inside(item,man_bbox):\n","                  face_bbox = item\n","                  face_front = True\n","                  break\n","            # write out the facebbox and the man_bbox\n","            data = {\n","                'time_stamp': time_stamp,\n","                'boss_pos': man_bbox,\n","                'face_pos': face_bbox,\n","                'face_front':face_front\n","            }\n","            \n","            jsonList.append(data)\n","            time_stamp+=1\n","      else:\n","        jsonfile = {\n","            \"time_gap_per_stamp\": round(1/gap,2),\n","            \"resolution\":jsonResolu,\n","            \"data\":jsonList\n","        }\n","        json_str = json.dumps(jsonfile)\n","        outfile.write(json_str)\n","        print(\"game file generated\")\n","        break\n","if __name__ == \"__main__\":\n","  warnings.filterwarnings(\"ignore\")\n","  generate_game_file('lec_sample.mp4')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":122,"referenced_widgets":["3a1bc344451d4cd583130d7f1af0c3e9","2625562a9167427f80f3e95880b40d55","5e2766d4c4d54c8a894a231975e0f92c","907cfedb4a9a4f71ab2da34bec24a647","e9f003dab6ef4a50a6b4f94e35035c99","b78af9050ff345d0a0270488b79a9210","94ef21454bd54f7ab18649e26753f5c3","d201a03b15ba4b0aaff67778030e59f3","957c0df153804cf5940a1ee0513663ed","38d3c9389d5b4afcbd21cea08fcc1429","2983f400eee6491391c311db9a0ecea0"]},"id":"3-KKQXyidjBq","executionInfo":{"status":"ok","timestamp":1645951331128,"user_tz":0,"elapsed":163561,"user":{"displayName":"scott huang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhHOANt4Vlu9JS3VBKWPOfNJfvVYqI1ySmaWmRk=s64","userId":"06489357941762676089"}},"outputId":"c189fd2e-afcd-4a9c-e363-75fb3c1d6f82"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading: \"https://folk.ntnu.no/haakohu/RetinaFace_mobilenet025.pth\" to /root/.cache/torch/hub/checkpoints/RetinaFace_mobilenet025.pth\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3a1bc344451d4cd583130d7f1af0c3e9","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0.00/1.71M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["game file generated\n"]}]}]}